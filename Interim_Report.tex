\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{geometry}
\usepackage{float}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}

\geometry{margin=1in}

% Code listing style
\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    stringstyle=\color{red},
    commentstyle=\color{green},
    numbers=left,
    numberstyle=\tiny,
    breaklines=true,
    frame=single,
    captionpos=b
}

\title{Compress-Transfer-Decompress for LLM Serving:\\
cuSZp-Enabled CPU-GPU Data Pipeline in vLLM\\
\large Interim Report}
\author{Your Name\\Student ID: Your Student ID}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This interim report presents the progress of integrating cuSZp compression library into the vLLM framework to optimize CPU-GPU data transfer for Large Language Model (LLM) serving. The project aims to reduce PCIe bandwidth bottlenecks by implementing a compress-transfer-decompress pipeline for KV cache swapping operations. This report documents the completed work including code framework design, integration architecture, and benchmarking infrastructure. The implementation is currently in the code development phase and requires GPU environment testing for validation and performance evaluation.
\end{abstract}

\tableofcontents
\newpage

\section{Introduction}

\subsection{Background and Motivation}

Large Language Models (LLMs) have revolutionized natural language processing, but their deployment faces significant challenges, particularly in memory management and data transfer efficiency. The vLLM framework \cite{vllm} addresses memory efficiency through PagedAttention, which enables efficient KV cache management by swapping blocks between GPU and CPU memory.

However, CPU-GPU data transfer over PCIe remains a bottleneck. Modern GPUs can achieve compute throughputs exceeding 1000 TFLOPS, but PCIe bandwidth is typically limited to 16-32 GB/s (PCIe 3.0/4.0). This bandwidth limitation becomes critical when KV cache blocks need to be swapped between GPU and CPU memory during inference.

Error-bounded lossy compression offers a promising solution. By compressing data before transfer and decompressing after transfer, we can effectively reduce the amount of data transmitted over PCIe, potentially achieving 2-10x compression ratios while maintaining acceptable accuracy for LLM inference.

\subsection{Project Objectives}

The primary objectives of this project are:

\begin{enumerate}
    \item Integrate cuSZp \cite{cuszp}, an ultra-fast GPU error-bounded lossy compression framework, into vLLM's CPU-GPU swap mechanism
    \item Implement a compress-transfer-decompress pipeline for KV cache blocks
    \item Evaluate the performance improvements in terms of transfer time reduction and overall inference throughput
    \item Maintain compatibility with existing vLLM features while ensuring acceptable accuracy loss
\end{enumerate}

\subsection{Project Scope}

This project focuses on:
\begin{itemize}
    \item Modifying vLLM's swap operations to use compression
    \item Creating a C++ wrapper for cuSZp with Python bindings
    \item Implementing asynchronous compression/decompression using CUDA streams
    \item Developing benchmarking infrastructure for performance evaluation
\end{itemize}

\section{Related Work}

\subsection{vLLM and PagedAttention}

vLLM \cite{vllm} is a high-throughput LLM serving framework that implements PagedAttention, a memory management technique inspired by virtual memory and paging in operating systems. PagedAttention allows non-contiguous storage of KV cache blocks, enabling efficient memory utilization and dynamic allocation.

The framework uses swap operations to move KV cache blocks between GPU and CPU memory:
\begin{itemize}
    \item \texttt{swap\_out\_blocks\_to\_host}: Transfers blocks from GPU to CPU
    \item \texttt{swap\_in\_blocks\_from\_host}: Transfers blocks from CPU to GPU
\end{itemize}

These operations are critical bottlenecks when GPU memory is limited and frequent swapping is required.

\subsection{cuSZp Compression Framework}

cuSZp \cite{cuszp} is a GPU-accelerated error-bounded lossy compression framework designed for scientific computing. Key features include:

\begin{itemize}
    \item \textbf{High Performance}: Compression speeds of 300-400 GB/s and decompression speeds of 400-600 GB/s on A100 GPUs
    \item \textbf{Error Bounded}: Guarantees compression errors within specified bounds (absolute or relative)
    \item \textbf{Multiple Encoding Modes}: Supports fixed, plain, and outlier encoding modes for different data characteristics
    \item \textbf{Asynchronous Execution}: Supports CUDA streams for overlapping compression with computation
\end{itemize}

The framework is particularly suitable for this project because:
\begin{enumerate}
    \item It operates entirely on GPU, avoiding CPU-GPU transfers for compression operations
    \item It provides configurable error bounds suitable for LLM inference accuracy requirements
    \item It achieves high compression ratios (2-10x) for smooth data typical of neural network activations
\end{enumerate}

\section{System Architecture}

\subsection{Overall Design}

The integration follows a modular architecture with three main components:

\begin{enumerate}
    \item \textbf{cuSZp Wrapper (C++)}: Low-level interface to cuSZp library with PyTorch tensor support
    \item \textbf{Compression Pipeline (Python)}: High-level compression manager integrated with vLLM
    \item \textbf{Benchmarking Infrastructure}: Performance testing and evaluation tools
\end{enumerate}

\subsection{Compression Pipeline Architecture}

The compression pipeline modifies vLLM's swap operations as follows:

\subsubsection{Device-to-Host (D2H) Compression Flow}

\begin{algorithm}[H]
\caption{D2H Compressed Swap}
\begin{algorithmic}[1]
\REQUIRE GPU KV cache block, compression configuration
\ENSURE Compressed data in CPU memory
\STATE Extract block from GPU KV cache
\STATE Allocate compression buffer on GPU
\STATE Compress block on GPU (asynchronous, using CUDA stream)
\STATE Transfer compressed data to CPU (reduced data size)
\STATE Store compressed data in CPU memory
\IF{compression fails}
    \STATE Fallback to uncompressed swap
\ENDIF
\end{algorithmic}
\end{algorithm}

\subsubsection{Host-to-Device (H2D) Decompression Flow}

\begin{algorithm}[H]
\caption{H2D Compressed Swap}
\begin{algorithmic}[1]
\REQUIRE Compressed data in CPU memory
\ENSURE Decompressed block in GPU KV cache
\STATE Read compressed data from CPU memory
\STATE Transfer compressed data to GPU (reduced data size)
\STATE Allocate decompression buffer on GPU
\STATE Decompress data on GPU (asynchronous, using CUDA stream)
\STATE Write decompressed block to GPU KV cache
\IF{decompression fails}
    \STATE Fallback to uncompressed swap
\ENDIF
\end{algorithmic}
\end{algorithm}

\subsection{Key Design Decisions}

\subsubsection{Asynchronous Execution}

Compression and decompression operations are performed asynchronously using CUDA streams, allowing overlap with:
\begin{itemize}
    \item Data transfers (using pinned memory)
    \item Other GPU computations
    \item CPU-side operations
\end{itemize}

This design maximizes GPU utilization and minimizes overall latency.

\subsubsection{Error Handling and Fallback}

The implementation includes robust error handling:
\begin{itemize}
    \item Automatic fallback to uncompressed swap if compression/decompression fails
    \item Configurable error bounds to balance compression ratio and accuracy
    \item Logging and monitoring for debugging and optimization
\end{itemize}

\subsubsection{Memory Management}

The wrapper implements memory pool management for compression buffers to:
\begin{itemize}
    \item Reduce allocation overhead
    \item Minimize memory fragmentation
    \item Support efficient reuse of buffers
\end{itemize}

\section{Implementation Details}

\subsection{cuSZp Wrapper (C++)}

The C++ wrapper (\texttt{cuszp\_wrapper.h/cpp}) provides a clean interface to cuSZp:

\begin{lstlisting}[caption=CuSZpWrapper Class Interface]
class CuSZpWrapper {
public:
    struct CompressionConfig {
        float error_bound = 1e-4f;
        bool use_relative_error = true;
        cuszp_dim_t processing_dim = CUSZP_DIM_1;
        cuszp_mode_t encoding_mode = CUSZP_MODE_PLAIN;
        cuszp_type_t data_type = CUSZP_TYPE_FLOAT;
    };
    
    bool compress(
        torch::Tensor input_tensor,
        torch::Tensor& compressed_buffer,
        size_t& compressed_size,
        cudaStream_t stream = nullptr
    );
    
    bool decompress(
        torch::Tensor compressed_buffer,
        size_t compressed_size,
        torch::Tensor output_tensor,
        cudaStream_t stream = nullptr
    );
};
\end{lstlisting}

\subsection{Python Bindings}

Pybind11 is used to expose the C++ wrapper to Python:

\begin{lstlisting}[caption=Python Binding Example]
PYBIND11_MODULE(cuszp_wrapper_cpp, m) {
    py::class_<CuSZpWrapper::CompressionConfig>(m, "CompressionConfig")
        .def(py::init<>())
        .def_readwrite("error_bound", ...)
        .def_readwrite("encoding_mode", ...);
    
    py::class_<CuSZpWrapper>(m, "CuSZpWrapper")
        .def(py::init<...>())
        .def("compress", ...)
        .def("decompress", ...);
}
\end{lstlisting}

\subsection{Compression Pipeline (Python)}

The \texttt{CompressedSwapManager} class manages compressed swap operations:

\begin{lstlisting}[caption=CompressedSwapManager Core Methods]
class CompressedSwapManager:
    def __init__(self, enable_compression=True, 
                 error_bound=1e-4, encoding_mode="plain"):
        # Initialize cuSZp wrapper
        config = CompressionConfig(...)
        self.compressor = CuSZpWrapper(config, device_id)
        self.compression_stream = torch.cuda.Stream()
        self.decompression_stream = torch.cuda.Stream()
    
    def swap_out_blocks_to_host_compressed(self, ...):
        # Compress on GPU, transfer to CPU
        with torch.cuda.stream(self.compression_stream):
            compressed_size = ctypes.c_size_t(0)
            success = self.compressor.compress(...)
            compressed_cpu = compressed_buffer.cpu()
    
    def swap_in_blocks_from_host_compressed(self, ...):
        # Transfer from CPU, decompress on GPU
        compressed_gpu = compressed_cpu.to(device)
        with torch.cuda.stream(self.decompression_stream):
            success = self.compressor.decompress(...)
\end{lstlisting}

\subsection{Build System}

The project uses CMake for building:

\begin{itemize}
    \item \texttt{cuSZp/CMakeLists.txt}: Builds cuSZp library
    \item \texttt{integration/cuszp\_wrapper/CMakeLists.txt}: Builds wrapper and Python bindings
    \item \texttt{build.sh}: Automated build script for convenience
\end{itemize}

\section{Testing and Benchmarking Infrastructure}

\subsection{Baseline Profiling}

The \texttt{baseline\_profiling.py} script measures baseline performance:

\begin{itemize}
    \item \textbf{H2D Transfer}: Measures host-to-device transfer bandwidth
    \item \textbf{D2H Transfer}: Measures device-to-host transfer bandwidth
    \item \textbf{Async Overlap}: Evaluates asynchronous transfer and computation overlap
\end{itemize}

\subsection{Compression Benchmarking}

The \texttt{compression\_benchmark.py} script evaluates compression performance:

\begin{itemize}
    \item \textbf{Compression Speed}: Measures compression throughput (GB/s)
    \item \textbf{Decompression Speed}: Measures decompression throughput (GB/s)
    \item \textbf{Compression Ratio}: Calculates data size reduction
    \item \textbf{Error Analysis}: Evaluates compression errors (max, mean)
    \item \textbf{Error Bound Sweep}: Tests performance across different error bounds
\end{itemize}

\subsection{Expected Performance Metrics}

Based on cuSZp performance characteristics and typical PCIe bandwidth:

\begin{table}[H]
\centering
\caption{Expected Performance Improvements}
\begin{tabular}{lcc}
\toprule
Metric & Baseline & With Compression \\
\midrule
D2H Transfer Time & 100\% & 30-70\% (reduction) \\
H2D Transfer Time & 100\% & 30-70\% (reduction) \\
Compression Speed & N/A & 300-400 GB/s \\
Decompression Speed & N/A & 400-600 GB/s \\
Compression Ratio & 1x & 2-10x \\
\bottomrule
\end{tabular}
\end{table}

\section{Completed Work}

\subsection{Phase 1: Environment Analysis and Baseline Setup}

\begin{itemize}
    \item Analyzed vLLM code structure and CPU-GPU swap mechanisms
    \item Identified integration points (\texttt{swap\_out\_blocks\_to\_host}, \texttt{swap\_in\_blocks\_from\_host})
    \item Created project documentation structure
    \item Designed integration architecture
\end{itemize}

\subsection{Phase 2: cuSZp Integration Preparation}

\begin{itemize}
    \item Analyzed cuSZp API and compression workflow
    \item Designed C++ wrapper class (\texttt{CuSZpWrapper})
    \item Implemented compression and decompression interfaces
    \item Created Python bindings using pybind11
    \item Set up CMake build system
\end{itemize}

\subsection{Phase 3: Compression Pipeline Implementation}

\begin{itemize}
    \item Implemented \texttt{CompressedSwapManager} class
    \item Created compressed versions of swap operations
    \item Added asynchronous execution support using CUDA streams
    \item Implemented error handling and fallback mechanisms
    \item Created automated build script (\texttt{build.sh})
\end{itemize}

\subsection{Phase 4: Benchmarking Infrastructure}

\begin{itemize}
    \item Created baseline performance profiling script
    \item Created compression performance benchmark script
    \item Added Docker support for containerized deployment
    \item Created comprehensive documentation (integration guide, deployment guide)
\end{itemize}

\section{Current Status}

\subsection{Code Completion Status}

\begin{table}[H]
\centering
\caption{Implementation Status}
\begin{tabular}{lc}
\toprule
Component & Status \\
\midrule
cuSZp Wrapper (C++) & \checkmark Complete \\
Python Bindings & \checkmark Complete \\
Compression Pipeline & \checkmark Complete \\
Build System & \checkmark Complete \\
Benchmarking Scripts & \checkmark Complete \\
Documentation & \checkmark Complete \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Testing Status}

\begin{itemize}
    \item \textbf{Code Framework}: All code has been written and structured
    \item \textbf{Compilation}: Code structure is ready for compilation (requires GPU environment)
    \item \textbf{Runtime Testing}: Pending GPU environment access
    \item \textbf{Performance Evaluation}: Benchmarking scripts ready, awaiting execution
\end{itemize}

\section{Challenges and Solutions}

\subsection{Technical Challenges}

\subsubsection{Memory Management}

\textbf{Challenge}: Compressed data sizes vary, requiring dynamic buffer allocation.

\textbf{Solution}: Implemented buffer size estimation and dynamic allocation with memory pooling for efficiency.

\subsubsection{Integration with vLLM}

\textbf{Challenge}: vLLM's CPU cache structure expects uncompressed data.

\textbf{Solution}: Designed wrapper that can store compressed data with metadata, with fallback to uncompressed storage if needed.

\subsubsection{Asynchronous Execution}

\textbf{Challenge}: Ensuring proper synchronization between compression, transfer, and decompression.

\textbf{Solution}: Used CUDA streams with proper synchronization points and error checking.

\subsection{Development Challenges}

\subsubsection{GPU Environment Access}

\textbf{Challenge}: Limited access to GPU environments for testing during development.

\textbf{Solution}: Structured code to be testable in stages, with comprehensive error handling and logging for remote debugging.

\section{Next Steps}

\subsection{Immediate Next Steps}

\begin{enumerate}
    \item \textbf{GPU Environment Setup}: Obtain access to GPU server or cloud GPU instance
    \item \textbf{Compilation and Testing}: Compile cuSZp and wrapper, verify basic functionality
    \item \textbf{Unit Testing}: Test compression/decompression with various data patterns
    \item \textbf{Integration Testing}: Integrate with vLLM and test end-to-end functionality
\end{enumerate}

\subsection{Performance Evaluation}

\begin{enumerate}
    \item Run baseline profiling to establish performance metrics
    \item Run compression benchmarks to measure compression performance
    \item Conduct end-to-end tests with real LLM inference workloads
    \item Compare performance with and without compression
    \item Analyze accuracy impact of compression on model outputs
\end{enumerate}

\subsection{Future Enhancements}

\begin{enumerate}
    \item \textbf{Optimization}: Fine-tune compression parameters for different model architectures
    \item \textbf{Memory Management}: Implement more sophisticated memory pool management
    \item \textbf{Metadata Storage}: Design efficient storage for compression metadata
    \item \textbf{Multi-GPU Support}: Extend support for multi-GPU environments
    \item \textbf{Dynamic Adaptation}: Implement adaptive compression based on workload characteristics
\end{enumerate}

\section{Project Timeline}

\begin{table}[H]
\centering
\caption{Project Timeline}
\begin{tabular}{lll}
\toprule
Phase & Duration & Status \\
\midrule
Phase 1: Environment Analysis & Weeks 1-3 & \checkmark Complete \\
Phase 2: cuSZp Integration Prep & Weeks 4-6 & \checkmark Complete \\
Phase 3: Pipeline Implementation & Weeks 7-11 & \checkmark Complete \\
Phase 4: Benchmarking Framework & Weeks 12-15 & \checkmark Complete \\
Phase 5: Testing \& Evaluation & Weeks 16-20 & \textrightarrow In Progress \\
Phase 6: Optimization \& Analysis & Weeks 21-24 & Pending \\
\bottomrule
\end{tabular}
\end{table}

\section{Conclusion}

This interim report documents the substantial progress made in integrating cuSZp compression into vLLM's CPU-GPU data pipeline. The code framework is complete, including:

\begin{itemize}
    \item C++ wrapper for cuSZp with PyTorch integration
    \item Python compression pipeline integrated with vLLM architecture
    \item Comprehensive benchmarking infrastructure
    \item Complete build system and documentation
\end{itemize}

The project is now ready for GPU environment testing and performance evaluation. The modular architecture and comprehensive error handling provide a solid foundation for optimization and refinement based on empirical results.

The next phase will focus on validating the implementation, measuring performance improvements, and optimizing compression parameters for real-world LLM serving scenarios.

\section{Acknowledgments}

The author acknowledges the following open-source projects and resources:
\begin{itemize}
    \item vLLM project for the LLM serving framework
    \item cuSZp authors for the compression framework
    \item PyTorch and pybind11 communities for development tools
\end{itemize}

\bibliographystyle{ieeetr}
\begin{thebibliography}{9}

\bibitem{vllm}
Kwon, W., et al. (2023). Efficient memory management for large language model serving with pagedattention. \textit{Proceedings of the 29th Symposium on Operating Systems Principles (SOSP'23)}.

\bibitem{cuszp}
Huang, Y., et al. (2023). cuSZp: An Ultra-fast GPU Error-bounded Lossy Compression Framework. \textit{Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC'23)}.

\end{thebibliography}

\appendix

\section{Project Structure}

\begin{verbatim}
FYP/
├── cuSZp/                    # cuSZp compression library
├── integration/              # Integration code
│   ├── cuszp_wrapper/      # C++ wrapper and Python bindings
│   └── compression_pipeline/ # Python compression pipeline
├── benchmarks/              # Performance testing scripts
├── docs/                    # Documentation
└── docker/                  # Docker deployment configuration
\end{verbatim}

\section{Key Files}

\begin{itemize}
    \item \texttt{integration/cuszp\_wrapper/cuszp\_wrapper.h}: C++ wrapper header
    \item \texttt{integration/cuszp\_wrapper/cuszp\_wrapper.cpp}: C++ wrapper implementation
    \item \texttt{integration/cuszp\_wrapper/pybind11\_bindings.cpp}: Python bindings
    \item \texttt{integration/compression\_pipeline/compressed\_swap.py}: Compression pipeline
    \item \texttt{benchmarks/baseline\_profiling.py}: Baseline performance tests
    \item \texttt{benchmarks/compression\_benchmark.py}: Compression performance tests
\end{itemize}

\end{document}

